**Планирование логирования**

Системы, которые покрываем логами в первую очередь: SHOP API, CRM API и MES API

На уровне INFO предлагаю следующий формат логирования:
1. Для любого запроса внешних систем фиксируем лог в формате: [дата-время][REQ/RESP][endpoint], и далее все idшники, которые были переданы и помогут в разборе кейса в случае проблем. Если нет sensitive данных, то можно сохранять всё body запроса + хэдеры (в рамках разумного). Если запрос касается заказа, обязательным полем будет статус - в случае REQ это будет текущий, в случае RESP это будет новый статус (измененый или оставшийся)
2. Внутри сервисов для отслеживания корректности прохождения бизнес-логик отслеживаем по ID интересующих объектов (заказ, клиент, 3D модель и т.д.) время и смену статусов. Например, в сервисе MES API после взятия в обработку заказа должны быть записи, по которым видны переходы по статусам MANUFACTURING_APPROVED -> MANUFACTURING_STARTED -> MANUFACTURING_COMPLETED -> PACKAGING -> SHIPPED. И в конце 200 OK от очереди или CRM API (в зависимости от того, куда отправляем данные об отправке заказа).

**Мотивация**
1. Выявление ошибок (Error Rate). Понимать, где сейчас происходят ошибки, какие именно. Напрямую повлияет на стабильность системы.
2. Сокращение времени разбора инцидентов (MTTD, MTTR). За счет понимания, что в конкретный момент ушло из системы, что пришло в другую, как данные обрабатывались.
3. Появится возможность гарантировать SLA по решению проблем и выполнению заказов (SLA Rate, CSAT)

**Предлагаемое решение**

В формате [drawio](/task4/jewerly_c4_model.drawio)

![](/task4/jewerly_c4_model.PNG)

1. Fluent Bit - сбор и отправка логов
2. Loki - сбор, обработка и сохранение логов
3. S3 - хранилище логов. Для логов создается отдельный бакет, со своими доступами. Доступ имею только devops + ИБ
4. Grafana - через Explore просмотр логов (в т.ч. с использованием trace id, если успели настроить ранее). Через Grafana настраиваем доступы на возможность просмотра логов в разрезе сервисов (разрабы), редактирование дашбордов (админы/devops). Если используется Active Diectory, можно подумать над Grafana + LDAP.

При передаче логов используется шифрование. Средствами FluentBit маскируем данные через замену или удаление чувствительных данных, чтобы в Loki данные пришли уже в безопасном виде. Срок хранения данных и правила удаления настраиваются через Loki:
compactor.retention_enabled - разрешаем compactor удаление
limits_config.retention_period - устанавливаем длительность хранения данных
compactor.retention_delete_delay (опционально) - защита от случайного удаления

**Превращение в систему анализа логов**

В зависимости от выбранного инструмента можно настроить либо средствами инструмента, либо самостогятельная несложная доработка.
1. Алерт на любые ошибки, происходящие в момент смены статуса заказа. Прямой риск "зависания" заказа
2. Зависшие заказы - ввести порог на время нахождения в определенных статусах и анализировать это самое время
3. Настройка и группировка 400 и 500 ошибок, задание порогов и формирование алертов в случае превышения этого порога
4. Алертинг на значение ниже или выше определенного диапазона порога, что будет говорить о том, что поступает аномально низкое или высокое количество запросов

Алертинг можно настроить и в корпоративный мессенджер, и в Телеграм, и на почту

**Выбор технологии**

| Критерий | ELK | OpenSearch | Splunk | Loki |
| ---------- | ---------- | ---------- | ---------- | ---------- |
| Лицензия | Elastic License 2.0 | Apache 2.0 | Проприетарная | AGPL v3 |
| Стоимость | Базовый функционал бесплатно, есть платные расширенные функции | Бесплатно | Платно и дорого, скорее для созревших крупных корпораций | Бесплатно |
| Безопасность | RBAC, TLS | Встроенный плагин (RBAC, TLS, аудит) | enterprise, сертификации | Базовая |
| Масштабируемость | Горизонтальная, высокая | Горизонтальная, высокая | Горизонтальная, наилучшая | Горизонтальная, высокая |
| Сложность настройки | Высокая | Средняя | Низкая | Низкая |
| Поддержка | Сообщество + платная поддержка для платных функций | Сообщество | Платная, зависит от тарифа | Сообщество |

На текущем этапе подойдут Loki или OpenSearch (скорее первое) - просто настраивается, сразу можно будет получить узкие места, над которыми нужно будет запускать работу. В процессе исправлений можно запустить переход на целевое решение в  виде ELK. Splunk можно будет рассмотреть при дальнейшем расширении компании и когда в компании появится определенная культура работа с инфидентами, иначе это будут деньги на ветер.
